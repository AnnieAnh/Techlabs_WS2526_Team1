{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee9VcppUHY1Y"
      },
      "outputs": [],
      "source": [
        "# you have to use python 3.12 to get it working\n",
        "\n",
        "!pip uninstall numpy pandas jobspy -y\n",
        "!pip install numpy==1.26.4\n",
        "!pip install python-jobspy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YUZMMDf6N7sI",
        "outputId": "ff13f9e5-24a1-46dc-8018-eb4aa7454f81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m‚úÖ INFO    ‚úì Found 30 jobs\u001b[0m\n",
            "\u001b[32m‚úÖ INFO üîé [4/8] INDEED üá©üá™ DE: 'Dateningenieur' in Berlin\u001b[0m\n",
            "\u001b[33m‚ö†Ô∏è WARNING    ‚úó No jobs found\u001b[0m\n",
            "\u001b[32m‚úÖ INFO üîé [5/8] INDEED üá¨üáß EN: 'software engineer' in Munich\u001b[0m\n",
            "\u001b[32m‚úÖ INFO    ‚úì Found 30 jobs\u001b[0m\n",
            "\u001b[32m‚úÖ INFO üîé [6/8] INDEED üá©üá™ DE: 'Softwareentwickler' in Munich\u001b[0m\n",
            "\u001b[32m‚úÖ INFO    ‚úì Found 16 jobs\u001b[0m\n",
            "\u001b[32m‚úÖ INFO üîé [7/8] INDEED üá¨üáß EN: 'data engineer' in Munich\u001b[0m\n",
            "\u001b[32m‚úÖ INFO    ‚úì Found 30 jobs\u001b[0m\n",
            "\u001b[32m‚úÖ INFO üîé [8/8] INDEED üá©üá™ DE: 'Dateningenieur' in Munich\u001b[0m\n",
            "\u001b[33m‚ö†Ô∏è WARNING    ‚úó No jobs found\u001b[0m\n",
            "\u001b[32m‚úÖ INFO \n",
            "üßπ INDEED: Removed 26 duplicates (140 ‚Üí 114)\u001b[0m\n",
            "\u001b[32m‚úÖ INFO \n",
            "üíæ Saving INDEED data...\u001b[0m\n",
            "\u001b[32m‚úÖ INFO    ‚úì CSV saved: Raw_Jobs_INDEED_2026-01-29_15-38-39.csv (114 jobs)\u001b[0m\n",
            "\u001b[32m‚úÖ INFO    ‚úì JSON saved: Raw_Jobs_INDEED_2026-01-29_15-38-39.json\u001b[0m\n",
            "\u001b[32m‚úÖ INFO \n",
            "================================================================================\u001b[0m\n",
            "\u001b[32m‚úÖ INFO ‚úÖ SCRAPING COMPLETE!\u001b[0m\n",
            "\u001b[32m‚úÖ INFO ================================================================================\u001b[0m\n",
            "\u001b[32m‚úÖ INFO ‚è±Ô∏è  Duration: 31.0s\u001b[0m\n",
            "\u001b[32m‚úÖ INFO \n",
            "üìä INDEED:\u001b[0m\n",
            "\u001b[32m‚úÖ INFO    ‚úì Successful searches: 6\u001b[0m\n",
            "\u001b[32m‚úÖ INFO    ‚úó Failed searches: 2\u001b[0m\n",
            "\u001b[32m‚úÖ INFO    üì¶ Jobs collected: 140 raw ‚Üí 114 unique\u001b[0m\n",
            "\u001b[32m‚úÖ INFO \n",
            "üì¶ TOTAL UNIQUE JOBS: 114\u001b[0m\n",
            "\u001b[32m‚úÖ INFO \n",
            "üìÅ SAVED FILES:\u001b[0m\n",
            "\u001b[32m‚úÖ INFO \n",
            "INDEED:\u001b[0m\n",
            "\u001b[32m‚úÖ INFO    üìÑ Raw_Jobs_INDEED_2026-01-29_15-38-39.csv\u001b[0m\n",
            "\u001b[32m‚úÖ INFO    üìÑ Raw_Jobs_INDEED_2026-01-29_15-38-39.json\u001b[0m\n",
            "\u001b[32m‚úÖ INFO \n",
            "üìÇ Location: c:\\Users\\ibrah\\Techlabs\\DS\\it-in-de\\job_data\u001b[0m\n",
            "\u001b[32m‚úÖ INFO ================================================================================\u001b[0m\n",
            "\u001b[32m‚úÖ INFO üìã Metadata saved: Metadata_2026-01-29_15-38-40.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "German IT Job Scraper - Bilingual (English + German)\n",
        "Scrapes jobs in both English and German\n",
        "\"\"\"\n",
        "\n",
        "import csv\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import pandas as pd\n",
        "from jobspy import scrape_jobs\n",
        "\n",
        "# ============================================================================\n",
        "# COLORED LOGGING\n",
        "# ============================================================================\n",
        "\n",
        "class ColoredFormatter(logging.Formatter):\n",
        "    \"\"\"Custom formatter with colors\"\"\"\n",
        "\n",
        "    COLORS = {\n",
        "        'DEBUG': '\\033[36m',\n",
        "        'INFO': '\\033[32m',\n",
        "        'WARNING': '\\033[33m',\n",
        "        'ERROR': '\\033[31m',\n",
        "        'CRITICAL': '\\033[35m',\n",
        "        'RESET': '\\033[0m',\n",
        "    }\n",
        "\n",
        "    EMOJI = {\n",
        "        'DEBUG': 'üîç',\n",
        "        'INFO': '‚úÖ',\n",
        "        'WARNING': '‚ö†Ô∏è',\n",
        "        'ERROR': '‚ùå',\n",
        "        'CRITICAL': 'üî•',\n",
        "    }\n",
        "\n",
        "    def format(self, record):\n",
        "        levelname = record.levelname\n",
        "        color = self.COLORS.get(levelname, self.COLORS['RESET'])\n",
        "        emoji = self.EMOJI.get(levelname, '')\n",
        "        record.levelname = f\"{emoji} {levelname}\"\n",
        "        message = super().format(record)\n",
        "        return f\"{color}{message}{self.COLORS['RESET']}\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ScraperConfig:\n",
        "    \"\"\"Configuration for the job scraper\"\"\"\n",
        "\n",
        "    # Job boards to scrape\n",
        "    job_boards: List[str] = field(default_factory=lambda: [\"indeed\", \"linkedin\"])\n",
        "    # job_boards: List[str] = field(default_factory=lambda: [\"linkedin\"])\n",
        "    # job_boards: List[str] = field(default_factory=lambda: [\"indeed\"])\n",
        "\n",
        "    # Cities to scrape\n",
        "    cities: List[str] = field(default_factory=lambda: [\n",
        "        \"Stuttgart\", \"Munich\", \"Berlin\", \"Potsdam\", \"Bremen\", \"Hamburg\",\n",
        "        \"Frankfurt\", \"Hanover\", \"Rostock\", \"Cologne\", \"Mainz\", \"Saarbr√ºcken\",\n",
        "        \"Dresden\", \"Magdeburg\", \"Kiel\", \"Erfurt\",\n",
        "        \"D√ºsseldorf\", \"Dortmund\", \"Essen\", \"Leipzig\", \"N√ºrnberg\",\n",
        "        \"Karlsruhe\", \"Mannheim\", \"Augsburg\", \"Wiesbaden\", \"M√ºnster\",\n",
        "        \"Bonn\", \"Freiburg\", \"Aachen\", \"Heidelberg\", \"Ulm\", \"Darmstadt\",\n",
        "        \"Regensburg\", \"Bielefeld\"\n",
        "    ])\n",
        "\n",
        "    # Bilingual job search keywords (English + German)\n",
        "    keywords: List[str] = field(default_factory=lambda: [\n",
        "        # English keywords\n",
        "        \"software engineer\",\n",
        "        \"software architect\",\n",
        "        \"software developer\",\n",
        "        \"data engineer\",\n",
        "        \"Data analyst\",\n",
        "        \"data scientist\",\n",
        "        \"BI developer\",\n",
        "        \"Cloud Engineer\",\n",
        "        \"Cloud architect\",\n",
        "        \"DevOps engineer\",\n",
        "        \"IT administrator\",\n",
        "        \"backend developer\",\n",
        "        \"frontend developer\",\n",
        "        \"full stack developer\",\n",
        "        \"Apps developer\",\n",
        "        \"SAP developer\",\n",
        "        \"machine learning engineer\",\n",
        "        \"AI engineer\",\n",
        "        \"cybersecurity engineer\",\n",
        "\n",
        "        # German keywords (Softwareentwickler, etc.)\n",
        "       \"Softwareentwickler\",\n",
        "       \"Softwarearchitekt\",\n",
        "       \"Softwareentwickler\",\n",
        "       \"Data Engineer\",\n",
        "       \"Datenanalyst\",\n",
        "       \"Data Scientist\",\n",
        "       \"BI-Entwickler\",\n",
        "       \"Cloud Engineer\",\n",
        "       \"Cloud-Architekt\",\n",
        "       \"DevOps Engineer\",\n",
        "       \"IT-Administrator\",\n",
        "       \"Backend Entwickler\",\n",
        "       \"Frontend Entwickler\",\n",
        "       \"FullStack Entwickler\",\n",
        "       \"Apps Entwickler\",\n",
        "       \"SAP Entwickler\",\n",
        "       \"Machine Learning Engineer\",\n",
        "       \"KI-Ingenieur\",\n",
        "       \"IT-Security Engineer\",\n",
        "    ])\n",
        "\n",
        "    # Scraping parameters\n",
        "    results_per_search: int = 150\n",
        "    job_type: str = \"fulltime\"\n",
        "    distance_km: int = 50\n",
        "    hours_old: int = 720 * 4  # 120 days\n",
        "    country: str = \"Germany\"\n",
        "\n",
        "    # LinkedIn specific\n",
        "    linkedin_fetch_description: bool = False\n",
        "\n",
        "    # Rate limiting\n",
        "    request_delay: int = 3\n",
        "    linkedin_delay: int = 6  # LinkedIn is more restrictive\n",
        "    retry_delay: int = 5\n",
        "    max_retries: int = 3\n",
        "\n",
        "    # Deduplication across languages\n",
        "    remove_duplicates: bool = True  # Remove duplicates found in both EN/DE searches\n",
        "\n",
        "    # Output directory\n",
        "    output_dir: Path = field(default_factory=lambda: Path(\"job_data\"))\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Create output directory\"\"\"\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# LOGGING SETUP\n",
        "# ============================================================================\n",
        "\n",
        "def setup_logging(config: ScraperConfig) -> logging.Logger:\n",
        "    \"\"\"Setup colored logging\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    log_file = config.output_dir / f\"Log_{timestamp}.log\"\n",
        "\n",
        "    logger = logging.getLogger(\"JobScraper\")\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "\n",
        "    if logger.handlers:\n",
        "        logger.handlers.clear()\n",
        "\n",
        "    # File handler\n",
        "    file_handler = logging.FileHandler(log_file, encoding='utf-8')\n",
        "    file_handler.setLevel(logging.DEBUG)\n",
        "    file_formatter = logging.Formatter(\n",
        "        '%(asctime)s | %(levelname)-8s | %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "    file_handler.setFormatter(file_formatter)\n",
        "\n",
        "    # Console handler\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setLevel(logging.INFO)\n",
        "    console_handler.setFormatter(ColoredFormatter('%(levelname)s %(message)s'))\n",
        "\n",
        "    logger.addHandler(file_handler)\n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "    return logger\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# UTILITY\n",
        "# ============================================================================\n",
        "\n",
        "def get_timestamp() -> str:\n",
        "    \"\"\"Get formatted timestamp\"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "\n",
        "def format_duration(seconds: float) -> str:\n",
        "    \"\"\"Format duration\"\"\"\n",
        "    if seconds < 60:\n",
        "        return f\"{seconds:.1f}s\"\n",
        "    elif seconds < 3600:\n",
        "        return f\"{seconds/60:.1f}m\"\n",
        "    else:\n",
        "        return f\"{seconds/3600:.1f}h\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MULTI-SITE BILINGUAL JOB SCRAPER\n",
        "# ============================================================================\n",
        "\n",
        "class BilingualJobScraper:\n",
        "    \"\"\"Fetch raw job data from multiple job boards in English and German\"\"\"\n",
        "\n",
        "    def __init__(self, config: ScraperConfig):\n",
        "        self.config = config\n",
        "        self.logger = setup_logging(config)\n",
        "        self.raw_data: Dict[str, List[pd.DataFrame]] = {\n",
        "            board: [] for board in config.job_boards\n",
        "        }\n",
        "        self.stats = {\n",
        "            'start_time': None,\n",
        "            'end_time': None,\n",
        "            'total_searches': 0,\n",
        "            'by_board': {board: {'successful': 0, 'failed': 0, 'jobs_raw': 0, 'jobs_deduped': 0}\n",
        "                        for board in config.job_boards}\n",
        "        }\n",
        "\n",
        "    def run(self, download_in_colab: bool = True) -> Dict[str, Tuple[Path, Path]]:\n",
        "        \"\"\"\n",
        "        Scrape jobs from all configured job boards\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping board name to (csv_path, json_path)\n",
        "        \"\"\"\n",
        "        self.stats['start_time'] = datetime.now()\n",
        "\n",
        "        self._print_header()\n",
        "\n",
        "        try:\n",
        "            # Scrape from each job board\n",
        "            for board in self.config.job_boards:\n",
        "                self.logger.info(f\"\\n{'='*80}\")\n",
        "                self.logger.info(f\"üåê Starting scrape from: {board.upper()}\")\n",
        "                self.logger.info(f\"{'='*80}\")\n",
        "\n",
        "                self._scrape_board(board)\n",
        "\n",
        "            # Save data for each board\n",
        "            saved_files = {}\n",
        "            for board in self.config.job_boards:\n",
        "                if self.raw_data[board]:\n",
        "                    combined_df = pd.concat(self.raw_data[board], ignore_index=True)\n",
        "                    self.stats['by_board'][board]['jobs_raw'] = len(combined_df)\n",
        "\n",
        "                    # Optional deduplication\n",
        "                    if self.config.remove_duplicates:\n",
        "                        original_count = len(combined_df)\n",
        "                        combined_df = combined_df.drop_duplicates(subset=['job_url'], keep='first')\n",
        "                        duplicates = original_count - len(combined_df)\n",
        "                        self.logger.info(f\"\\nüßπ {board.upper()}: Removed {duplicates} duplicates \"\n",
        "                                       f\"({original_count} ‚Üí {len(combined_df)})\")\n",
        "                        self.stats['by_board'][board]['jobs_deduped'] = len(combined_df)\n",
        "                    else:\n",
        "                        self.stats['by_board'][board]['jobs_deduped'] = len(combined_df)\n",
        "\n",
        "                    csv_path, json_path = self._save_data(combined_df, board)\n",
        "                    saved_files[board] = (csv_path, json_path)\n",
        "\n",
        "                    # Download if in Colab\n",
        "                    if download_in_colab:\n",
        "                        self._download_files(csv_path, json_path)\n",
        "                else:\n",
        "                    self.logger.warning(f\"No data collected from {board}\")\n",
        "\n",
        "            # Final summary\n",
        "            self.stats['end_time'] = datetime.now()\n",
        "            self._print_summary(saved_files)\n",
        "\n",
        "            return saved_files\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Fatal error: {str(e)}\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    def _print_header(self):\n",
        "        \"\"\"Print startup header\"\"\"\n",
        "        self.logger.info(\"=\"*80)\n",
        "        self.logger.info(\"üöÄ BILINGUAL GERMAN JOB SCRAPER (EN + DE)\")\n",
        "        self.logger.info(\"=\"*80)\n",
        "        self.logger.info(f\"üåê Job Boards: {', '.join(self.config.job_boards)}\")\n",
        "        self.logger.info(f\"üìç Cities: {len(self.config.cities)}\")\n",
        "        self.logger.info(f\"üîë Keywords: {len(self.config.keywords)} (English + German)\")\n",
        "        self.logger.info(f\"üìÖ Date Range: Last {self.config.hours_old//24} days\")\n",
        "\n",
        "        total_searches = len(self.config.job_boards) * len(self.config.cities) * len(self.config.keywords)\n",
        "        self.stats['total_searches'] = total_searches\n",
        "\n",
        "        self.logger.info(f\"üéØ Total searches: {total_searches}\")\n",
        "        self.logger.info(\"=\"*80)\n",
        "\n",
        "    def _scrape_board(self, board: str):\n",
        "        \"\"\"Scrape all cities and keywords for a specific job board\"\"\"\n",
        "        searches_for_board = len(self.config.cities) * len(self.config.keywords)\n",
        "        current_search = 0\n",
        "\n",
        "        for city in self.config.cities:\n",
        "            for keyword in self.config.keywords:\n",
        "                current_search += 1\n",
        "\n",
        "                # Detect language for logging\n",
        "                lang = \"üá©üá™ DE\" if self._is_german_keyword(keyword) else \"üá¨üáß EN\"\n",
        "\n",
        "                self.logger.info(\n",
        "                    f\"üîé [{current_search}/{searches_for_board}] \"\n",
        "                    f\"{board.upper()} {lang}: '{keyword}' in {city}\"\n",
        "                )\n",
        "\n",
        "                # Try with retries\n",
        "                for attempt in range(self.config.max_retries):\n",
        "                    try:\n",
        "                        jobs = self._scrape_single(board, city, keyword)\n",
        "\n",
        "                        if jobs is not None and len(jobs) > 0:\n",
        "                            self.logger.info(f\"   ‚úì Found {len(jobs)} jobs\")\n",
        "                            self.raw_data[board].append(jobs)\n",
        "                            self.stats['by_board'][board]['successful'] += 1\n",
        "                            break\n",
        "                        else:\n",
        "                            self.logger.warning(f\"   ‚úó No jobs found\")\n",
        "                            self.stats['by_board'][board]['failed'] += 1\n",
        "                            break\n",
        "\n",
        "                    except Exception as e:\n",
        "                        self.logger.error(f\"   ‚úó Attempt {attempt + 1} failed: {str(e)}\")\n",
        "                        if attempt < self.config.max_retries - 1:\n",
        "                            self.logger.info(f\"   ‚è≥ Retrying in {self.config.retry_delay}s...\")\n",
        "                            time.sleep(self.config.retry_delay)\n",
        "                        else:\n",
        "                            self.stats['by_board'][board]['failed'] += 1\n",
        "\n",
        "                # Board-specific delay\n",
        "                delay = self.config.linkedin_delay if board == \"linkedin\" else self.config.request_delay\n",
        "                time.sleep(delay)\n",
        "\n",
        "    def _is_german_keyword(self, keyword: str) -> bool:\n",
        "        \"\"\"Check if keyword is in German\"\"\"\n",
        "        german_indicators = [\n",
        "            'entwickler', 'ingenieur', 'wissenschaftler', 'administrator',\n",
        "            'spezialist', 'systemadministrator'\n",
        "        ]\n",
        "        return any(indicator in keyword.lower() for indicator in german_indicators)\n",
        "\n",
        "    def _scrape_single(self, board: str, city: str, keyword: str) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Execute a single job search\"\"\"\n",
        "        params = {\n",
        "            'site_name': [board],\n",
        "            'search_term': keyword,\n",
        "            'location': city,\n",
        "            'results_wanted': self.config.results_per_search,\n",
        "            'job_type': self.config.job_type,\n",
        "            'distance': self.config.distance_km,\n",
        "            'verbose': 0\n",
        "        }\n",
        "\n",
        "        # Add board-specific parameters\n",
        "        if board == \"indeed\":\n",
        "            params['country_indeed'] = self.config.country\n",
        "            params['hours_old'] = self.config.hours_old\n",
        "\n",
        "        elif board == \"linkedin\":\n",
        "            params['hours_old'] = self.config.hours_old\n",
        "            params['linkedin_fetch_description'] = self.config.linkedin_fetch_description\n",
        "\n",
        "        return scrape_jobs(**params)\n",
        "\n",
        "    def _save_data(self, df: pd.DataFrame, board: str) -> Tuple[Path, Path]:\n",
        "        \"\"\"Save data to CSV and JSON\"\"\"\n",
        "        timestamp = get_timestamp()\n",
        "        csv_path = self.config.output_dir / f\"Raw_Jobs_{board.upper()}_{timestamp}.csv\"\n",
        "        json_path = self.config.output_dir / f\"Raw_Jobs_{board.upper()}_{timestamp}.json\"\n",
        "\n",
        "        self.logger.info(f\"\\nüíæ Saving {board.upper()} data...\")\n",
        "\n",
        "        # Save CSV\n",
        "        df.to_csv(csv_path, index=False, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\")\n",
        "        self.logger.info(f\"   ‚úì CSV saved: {csv_path.name} ({len(df)} jobs)\")\n",
        "\n",
        "        # Save JSON\n",
        "        df.to_json(json_path, orient='records', indent=2, force_ascii=False)\n",
        "        self.logger.info(f\"   ‚úì JSON saved: {json_path.name}\")\n",
        "\n",
        "        return csv_path, json_path\n",
        "\n",
        "    def _download_files(self, csv_path: Path, json_path: Path):\n",
        "        \"\"\"Download files in Colab\"\"\"\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            files.download(str(csv_path))\n",
        "            files.download(str(json_path))\n",
        "        except ImportError:\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            self.logger.warning(f\"   Could not download: {str(e)}\")\n",
        "\n",
        "    def _print_summary(self, saved_files: Dict[str, Tuple[Path, Path]]):\n",
        "        \"\"\"Print final summary\"\"\"\n",
        "        duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds()\n",
        "\n",
        "        self.logger.info(\"\\n\" + \"=\"*80)\n",
        "        self.logger.info(\"‚úÖ SCRAPING COMPLETE!\")\n",
        "        self.logger.info(\"=\"*80)\n",
        "        self.logger.info(f\"‚è±Ô∏è  Duration: {format_duration(duration)}\")\n",
        "\n",
        "        # Stats by board\n",
        "        total_jobs = 0\n",
        "        for board in self.config.job_boards:\n",
        "            stats = self.stats['by_board'][board]\n",
        "            jobs = stats['jobs_deduped']\n",
        "            jobs_raw = stats['jobs_raw']\n",
        "            total_jobs += jobs\n",
        "\n",
        "            self.logger.info(f\"\\nüìä {board.upper()}:\")\n",
        "            self.logger.info(f\"   ‚úì Successful searches: {stats['successful']}\")\n",
        "            self.logger.info(f\"   ‚úó Failed searches: {stats['failed']}\")\n",
        "            self.logger.info(f\"   üì¶ Jobs collected: {jobs_raw} raw ‚Üí {jobs} unique\")\n",
        "\n",
        "        self.logger.info(f\"\\nüì¶ TOTAL UNIQUE JOBS: {total_jobs}\")\n",
        "\n",
        "        # Files\n",
        "        self.logger.info(\"\\nüìÅ SAVED FILES:\")\n",
        "        for board, (csv_path, json_path) in saved_files.items():\n",
        "            self.logger.info(f\"\\n{board.upper()}:\")\n",
        "            self.logger.info(f\"   üìÑ {csv_path.name}\")\n",
        "            self.logger.info(f\"   üìÑ {json_path.name}\")\n",
        "\n",
        "        self.logger.info(f\"\\nüìÇ Location: {self.config.output_dir.absolute()}\")\n",
        "        self.logger.info(\"=\"*80)\n",
        "\n",
        "        # Save overall metadata\n",
        "        self._save_metadata(total_jobs)\n",
        "\n",
        "    def _save_metadata(self, total_jobs: int):\n",
        "        \"\"\"Save scraping metadata\"\"\"\n",
        "        timestamp = get_timestamp()\n",
        "        metadata_path = self.config.output_dir / f\"Metadata_{timestamp}.json\"\n",
        "\n",
        "        metadata = {\n",
        "            'scrape_date': datetime.now().isoformat(),\n",
        "            'job_boards': self.config.job_boards,\n",
        "            'cities': self.config.cities,\n",
        "            'keywords': self.config.keywords,\n",
        "            'total_unique_jobs': total_jobs,\n",
        "            'bilingual': True,\n",
        "            'stats': self.stats\n",
        "        }\n",
        "\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2, default=str)\n",
        "\n",
        "        self.logger.info(f\"üìã Metadata saved: {metadata_path.name}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# USAGE EXAMPLES\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # # ========================================================================\n",
        "    # # EXAMPLE 1: Quick test (2 cities, few keywords, English + German)\n",
        "    # # ========================================================================\n",
        "    # print(\"\\nüß™ Quick bilingual test...\\n\")\n",
        "\n",
        "    # test_config = ScraperConfig(\n",
        "    #     job_boards=[\"indeed\"],  # Start with Indeed only\n",
        "    #     cities=[\"Berlin\", \"Munich\"],\n",
        "    #     keywords=[\n",
        "    #         \"software engineer\",      # English\n",
        "    #         \"Softwareentwickler\",     # German\n",
        "    #         \"data engineer\",          # English\n",
        "    #         \"Dateningenieur\",         # German\n",
        "    #     ],\n",
        "    #     results_per_search=30,\n",
        "    #     hours_old=168,  # Last 7 days\n",
        "    #     remove_duplicates=True\n",
        "    # )\n",
        "\n",
        "    # scraper = BilingualJobScraper(test_config)\n",
        "    # files = scraper.run(download_in_colab=True)\n",
        "\n",
        "\n",
        "    # ========================================================================\n",
        "    # EXAMPLE 2: Full bilingual run (Indeed + LinkedIn)\n",
        "    # ========================================================================\n",
        "    # print(\"\\nüéØ Full bilingual scrape (Indeed + LinkedIn)...\\n\")\n",
        "\n",
        "    scraper = BilingualJobScraper(ScraperConfig())\n",
        "    files = scraper.run(download_in_colab=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
